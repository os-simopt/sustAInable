{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852c8317-4f68-4ef1-8787-a0a562eb94f3",
   "metadata": {},
   "source": [
    "# Datenaufbereitung und -visualisierung mit modernen Python-Tools\n",
    "\n",
    "**Dr. Martin Felder, ZSW, im Oktober 2022**\n",
    "\n",
    "Python hat sich im Bereich Data Science als eine der erfolgreichsten Programmiersprachen durchgesetzt. Dies ist unter anderem der Entwicklung von immer ausgefeilteren Tools in den letzten Jahren zu verdanken, da die Sprache selber keine Arrays oder vektorielle Verarbeitung unterstützt.\n",
    "\n",
    "Das erste Tool und damit die Basis für (fast) alle darauffolgenden ist das Paket `numpy`. Es ermöglicht u.a. die Verarbeitung von Arrays mit definierten Datentypen, sowie das einfache Lesen und Schreiben binärer Daten. Wir werden in diesem Tutorial allerdings nicht näher auf `numpy` eingehen, es wird aber von den besprochenen Paketen implizit verwendet.\n",
    "\n",
    "Ein wichtiger Unterschied von `numpy` zu \"modernen\" Tools ist, dass Letztere bestimmte Metadaten mitverarbeiten und -abspeichern. Dies erlaubt eine viel komfortablere Filterung und Visualisierung von Daten. Außerdem vermeidet man viele Fehler, die ansonsten durch die unvermeidliche Manipulation von verschachtelten Indizes auftreten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2cec8-1bfa-4782-8460-f550051826d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gzip\n",
    "\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Die folgenden drei Abkürzungen haben sich in der Community etabliert:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e025be7-67c9-4d76-9a9e-4a8a5766b286",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tabellenartige Daten - `pandas`\n",
    "\n",
    "`pandas` erlaubt die Verarbeitung \"tabellenartiger\" Daten als `DataFrame`. Ein Beispiel zum Einlesen solcher Daten aus einem komprimierten CSV-File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd93be7-6807-433d-97b2-04fc4dc2cf02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gzip.open('temperatur.csv.gz', 'rb') as f:\n",
    "    df = pd.read_csv(f, index_col=\"time\", parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e324b9-4082-45a2-9053-0a69d52c2514",
   "metadata": {},
   "source": [
    "Hier sieht man bereits die mitverarbeiteten Metadaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d1a76-fb30-4d24-8d56-9b2b83bdc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns  # Eine Liste von Labels für die Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebaf86-fa01-40ea-a739-ae971ad435e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index  # Eine Liste von Labels für die Zeilen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a33c87-48b8-4a4d-80d0-50ea126bab6f",
   "metadata": {},
   "source": [
    "Diese Attribute lassen sich auch mit beliebigen Iterables überschreiben (sofern die Dimensionen stimmen). Die eigentlichen Daten liegen in Form eines `numpy` Arrays unter `df.values` vor, welches logischerweise die Dimension (# Zeilen, # Spalten) hat. Im Wesentlichen sind die Zeilen- und Spaltenindizes gleichberechtigt, man kann sie auch leicht durch Transposition vertauschen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a62ce-16e6-4a2a-8ea9-5b9fb2ba17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd20afd-f139-47d7-9cc9-65bb082d270d",
   "metadata": {},
   "source": [
    "Man kann den `DataFrame` aber auch als eine Art `dictionary` von Variablen ansehen, welches mit den Spaltennamen indiziert ist (die sich aus Conveniencegründen auch als Attribute schreiben lassen). Dies hat insofern seine Berechtigung, als die `dtypes` **pro Spalte** festgelegt sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e2a1e-6e80-43d8-995f-b8347d284647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"name\"].dtype, df.T02.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb72f01-6e32-4bba-ac9b-58c02de5553f",
   "metadata": {},
   "source": [
    "Darauf kommen wir später zurück. Spalten lassen sich natürlich entfernen oder umbenennen, oder wie bei einem `dictionary` hinzufügen (siehe Anleitung für Details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d65155-3082-4320-a01f-7b29e32ca2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Einstrahlung\", \"name\"]).rename(columns={\"T12\": \"T13\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08f88a-7962-4d68-b451-06bfeafbeb79",
   "metadata": {},
   "source": [
    "Mit den Attributen `.iloc` und `.loc` kann auf Subsets des `DataFrame`s zugegriffen werden, wobei `.iloc` einen numerischen Index erwartet und `.loc` einen, der die Labels von Zeilen und Spalten enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84867fa-1357-4dcc-a865-198851e98f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:2, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98ed34-ad38-4050-ab4d-0e1a8dbfb1b1",
   "metadata": {},
   "source": [
    "Die Indizierung erfolgt hier also wie bei `numpy`. Allerdings ist das Ergebnis wieder ein `DataFrame` - das gleiche Subset kann man sich natürlich auch als Array zurückgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52320707-baa7-4ac4-aec6-db24eb6dea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values[0:2, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff52331-fd88-4415-88fd-538c02a731e7",
   "metadata": {},
   "source": [
    "Die Indizierung mit Labels ist sehr mächtig und umfasst u.a. Typkonversionen usw. Dies umfassend darzustellen übersteigt den Rahmen des Tutorials und wird hiermit der [Anleitung](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#selection-by-label) überlassen. Ein einfaches Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e424aaf-3e0e-412a-bcc0-c5f78a8b48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"2022-06-29\":\"2022-06-29 00:20:00\", \"T01\":\"T04\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7bfce-1558-46e8-a0d0-d4b653b17df5",
   "metadata": {},
   "source": [
    "Durch die Mächtigkeit entstehen leider gelegentlich unintuitive Konventionen und/oder unerwartetes Verhalten. Bei Problemen bitte genau in der Anleitung nachlesen oder googlen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26439860-6af1-4f4a-9d41-247ae0c73317",
   "metadata": {},
   "source": [
    "### Einlesen bzw. Erzeugen von `DataFrame`s\n",
    "\n",
    "DataFrames lassen sich wie oben gezeigt mittels `pd.read_csv` aus CSV-Dateien einlesen. Der Parser ist sehr mächtig und macht es in den meisten Fällen unnötig, an den Dateien selber etwas zu verändern (im Gegensatz zum Excel-Parser...). Die nach meiner Erfahrung nützlichsten Argumente sind:\n",
    "\n",
    "* `index_col`: Name oder laufende Nummer (0-based) der Spalte, die als Index verwendet werden soll.\n",
    "* `parse_dates`: `True`, wenn der Index als Zeit interpretiert werden soll. Man kann auch mehrere Spalten angeben, die [zusammen interpretiert werden](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#datetime-handling).\n",
    "* `dayfirst`: Wichtig für europäische Datumsformate!\n",
    "* `delimiter`: Das Spaltentrennzeichen.\n",
    "* `skiprows`: Erlaubt das Überspringen von Headerzeilen am Anfang der Datei.\n",
    "* `header`: Spezifiziert, welche Zeile als Spaltennamen verwendet werden soll (oder `False`).\n",
    "* `encoding`: Falls die Datei Umlaute o.ä. enthält und unter Windows oder auf älteren Macs erzeugt wurde, muss hier wahrscheinlich entweder `'latin1'`, `'iso-8859-1'`, `'cp1252'` oder `'utf-16'` stehen... \n",
    "* `decimal`: Dezimaltrennzeichen, default `\".\"`. Für deutsche Zahlenformate `\",\"` einstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad10852-0b6b-4ded-a1b7-1dc33d7ecdda",
   "metadata": {},
   "source": [
    "Pandas kann mittels `pd.read_excel` auch direkt Excel-Tabellenblätter einlesen. Dies hat den Vorteil, dass Komplikationen wie das Encoding oder die Dezimaltrennzeichen wegfallen. Allerdings kann es Probleme geben, wenn die Zellentypen schon im Excel falsch eingestellt sind, z.B. Zahlen als Text. Ansonsten ist der [Parser](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#reading-excel-files) relativ ähnlich zum CSV-Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d123cf-3e97-4928-b014-92a72aa1fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot = pd.read_excel(\"solarthermie.xlsx\")\n",
    "sot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bcc9d-6ae9-45be-b918-d51f779385df",
   "metadata": {},
   "source": [
    "In diesem Beispiel liegt der Zeitstempel auf zwei Spalten verteilt vor. Da es sich aber nicht im Strings handelt, können sie nicht wie bei `parse_dates` zusammen interpretiert werden. Einen entsprechenden Zeitindex erhält man aber wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca84ebe-1706-45b4-bf5a-09f8e9875f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot = sot.set_index(sot.apply(lambda row: datetime.combine(row.date, row.time), 1))\n",
    "sot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8d080-126e-4c08-9804-520e4f57cb8f",
   "metadata": {},
   "source": [
    "Hier fällt auf, dass die Zeitstempel keine Zeitzone enthalten und der Index keinen Namen hat (oben hieß er `time`).\n",
    "\n",
    "#### Ärger mit der Lokalzeit\n",
    "\n",
    "Aber was passiert, wenn die ursprünglichen Daten in Lokalzeit abgespeichert sind, dies aber aus dem Zeitstempel-String nicht ersichtlich ist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed75508-3d60-46f8-9970-a16c23e789a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lz = pd.read_csv(\n",
    "    \"solarthermie_lokalzeit.csv\",\n",
    "    index_col=\"timestamp\",\n",
    "    parse_dates=True,\n",
    "    delimiter=\";\",\n",
    "    decimal=\",\",\n",
    ")\n",
    "lz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102de33d-9007-4aa5-a828-92513b2fd22d",
   "metadata": {},
   "source": [
    "Hier sind die Sprünge am Übergang zwischen Sommer- und Winterzeit leicht zu erkennen. **Dies bitte unbedingt überprüfen, wenn die Daten nicht eindeutig in UTC vorliegen!** Falsch interpretierte Zeitstempel sind Teufelswerk!\n",
    "\n",
    "Wenn die Zeitzone, in der die Daten vorliegen, bekannt ist, kann der Index mit `.tz_localize` als diese Zeitzone uminterpretiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebb1a2-7ef3-4e60-a663-9b4ea983660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lz = lz.tz_localize(\"Europe/Berlin\", ambiguous=\"infer\")\n",
    "lz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a729b6-8728-4d7d-9002-e4b4c257ba87",
   "metadata": {},
   "source": [
    "Es ist aber auf jeden Fall empfehlenswert **alles in UTC zu konvertieren und die Zeitzone dann zu entfernen**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb641487-c16c-4bac-a47c-77cb05b7f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lz.tz_convert(\"UTC\").tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d19c4d-7fbd-4b61-9f01-55eacd41615f",
   "metadata": {},
   "source": [
    "Wer häufig Probleme mit Zeitstempeln und Datumsberechnungen hat, sollte sich das speziell dafür optimierte Paket `arrow` ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e024a-e9cd-405b-9a17-4033e62665f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"Einstrahlung\").tz_localize(None)  # Dataframe aufräumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9c264-53b0-4b92-8edb-4f4b75bacf7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Statistische) Beschreibung von `DataFrame`s\n",
    "\n",
    "Für eine erste Übersicht reicht es meist, die Datentypen und Vollständigkeit der einzelnen Spalten zu kennen. Dazu dient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3220c-09cd-48da-a55d-f8b255c39bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8b77b-80d4-4714-86ec-585fc261bd08",
   "metadata": {},
   "source": [
    "Hier bietet es sich bereits an, die offenbar leeren Spalten zu entfernen, sowie die unvollständigen Zeilen (außer diese sollen interpoliert werden o.ä.). Wir zeigen hier nur den Befehl, weisen aber den Output keiner Variable zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35052be-9ddf-49e0-8489-720fe8b75992",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot.drop(columns=\"Energie\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c70f2-5e84-4d00-bb7e-6f705fb2da8e",
   "metadata": {},
   "source": [
    "Eine etwas ausführlichere statistische Beschreibung erhält man mit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d54066-1b09-4e13-9e34-7de0c970128c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sot.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc33a7-a421-4ad0-b72b-f96be1ba0fee",
   "metadata": {},
   "source": [
    "Die Stabilität der Mittelwerte aller Daten lässt sich z.B. mit dem ebenfalls implementierten Bootstrap-Plot untersuchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cef05-0491-4e90-930a-7abc1a178dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.plotting.bootstrap_plot(sot.TemperaturVL, fig=None, size=100, samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1152c8a-ea56-4334-ac4f-4bd322b94902",
   "metadata": {},
   "source": [
    "Wer es noch genauer wissen möchte, kann sich über das `pandas-profiling` Paket einen kompletten Statistikreport ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bf1ee-022c-4a62-9254-aad375d4cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabdbaa-ed8a-4d3e-a598-f9d1069e1fcc",
   "metadata": {},
   "source": [
    "### Selektion, Manipulation und Visualisierung\n",
    "\n",
    "Die Möglichkeiten zur Manipulation von DataFrames könnte man als eine Kombination von Excel- und SQL-Operationen ansehen, auch von der Mächtigkeit her. Wir kratzen hier kurz an der Oberfläche und sehen uns einige häufig verwendete Methoden an.\n",
    "\n",
    "Wichtig ist zunächst die Selektion der gewünschten Daten. Abgesehen von der Adressierung nach Labels und Integer-Indizes können natürlich auch Datenbereiche ausgewählt werden. Dazu gibt es verschiedene Methoden, die sich i.W. in der Performance unterscheiden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2277764-a429-446a-917e-5cddbfcc5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sot.iloc[(sot.DeltaTemp.values > 19) & (sot.Durchfluss.values > 2.7), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e118bb-cd94-4937-b326-2c8aeebbfacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sot[(sot.DeltaTemp > 19) & (sot.Durchfluss > 2.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aaf201-6532-4a7d-8803-c9add7059d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sot.query(\"DeltaTemp > 19 & Durchfluss > 2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7e9c6-ea83-424d-9f4f-7d5f97523453",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sot.where((sot.DeltaTemp > 19) & (sot.Durchfluss > 2.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582625a-90c2-4390-b782-a83f38d93419",
   "metadata": {},
   "source": [
    "Beim Operator `in` ist zu beachten, dass dieser sich nicht Index oder Daten bezieht, sondern auf die Spalten. Der DataFrame verhält sich in diesem Fall wie ein `dictionary` der Spalten, wie oben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e53d4c-7110-4939-9c3e-bee136888f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"sungo\" in df, pd.Timestamp(\"2022-06-28 05:28:45\") in df, \"T01\" in df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5762df8-8496-4f7d-a6cb-3a838dc84202",
   "metadata": {},
   "source": [
    "Ob gewisse Werte in den Daten enthalten sind, findet mal mit `.isin` heraus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e97d1-83d1-4d4a-9db3-96ab13417209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isin([\"sungo\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a11ac5-3779-46e8-ace9-5bdf14bd99d2",
   "metadata": {},
   "source": [
    "Downsampling von Zeitreihen ist mit Pandas kein Problem (zum Upsampling kommen wir weiter unten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8390e-e7fa-4311-8127-eaa8a1623d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.resample(\"h\").mean().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc965a11-d86b-4ff3-90f8-27fb5691cd78",
   "metadata": {},
   "source": [
    "Pandas enthält grundlegende Plotting-Funktionalität auf der Basis von Matplotlib. Das Statistikplot-Package `seaborn` ist ebenfalls mit Pandas kompatibel, wird aber aus Zeitgründen hier nicht behandelt. Einfache Plots erhält man aus einzelnen Spalten, oder unter Verwendung der Spaltennamen direkt aus dem ganzen `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f9734-92af-47a2-9ca5-ed8f612a825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.T01.plot(figsize=(20, 5), grid=True, label=\"T01\", legend=True)\n",
    "df.T03.plot(label=\"T03\", legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420461ba-ca48-41a1-9ecd-41d0775e6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x=\"T01\", y=\"T03\", c=\"T06\", alpha=0.3, cmap=\"rainbow\", figsize=(8, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3806cf5-6b2d-4fa3-a4a0-58506e818e3c",
   "metadata": {},
   "source": [
    "Eine weniger bekannte Alternative ist das Paket `hvplot`, das ein auf den ersten Blick sehr ähnliches Interface bietet, aber auf `bokeh` bzw. `holoviews` aufsetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ee87d-3b06-4014-8e37-441331fbb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.scatter(\n",
    "    x=\"T01\",\n",
    "    y=\"T03\",\n",
    "    c=\"T06\",\n",
    "    alpha=0.3,\n",
    "    cmap=\"rainbow\",\n",
    "    frame_width=500,\n",
    "    aspect=1,\n",
    "    clabel=\"T06 [°C]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260f927-e038-44e6-a77d-75e4d9753870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T01.hvplot(width=1200, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058c419-deed-4fa5-a2a7-2280b01928fb",
   "metadata": {},
   "source": [
    "Natürlich sind beide Lösungen nicht perfekt und haben jeweils ihre Macken. In diesem Tutorial werden wir uns auf `hvplot` konzentrieren. Wie bei allen high-level Plottingtools können manche Probleme nur auf den darunterliegenden Ebenen gelöst werden. Bei `hvplot` sind dies wie gesagt zunächst `holoviews`, noch weiter darunter liegt die `bokeh`-Schicht (die auf `D3.js` aufsetzt). Gewisse Hacks lassen sich oft leicht ergooglen, wie z.B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227326e-f76c-4aef-9f41-abcd20cdf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitachse auf deutsches Format unstellen\n",
    "from bokeh.models.formatters import DatetimeTickFormatter\n",
    "\n",
    "datefmt = DatetimeTickFormatter(\n",
    "    hours=\"%d.%m. %H:%M\", days=\"%d.%m.\", months=\"%Y-%m\", years=\"%Y-%m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2e53a-11cb-4103-866b-fb3fa439fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T01.hvplot(width=1200, grid=True, xformatter=datefmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f957a-bdc7-4805-8533-d42adf6bdef3",
   "metadata": {},
   "source": [
    "Ob man ein high-level Tool verwendet oder lieber z.B. auf der Matplotlib-Ebene bleibt, ist letztlich Geschmackssache. Nach meiner persönlichen Erfahrung sparen high-level Plots für schnelle Datensichtungen und -auswertungen nach einer gewissen Einarbeitung durchaus Zeit, auch wenn man gelegentlich an die Grenzen stößt und etwas Zeit auf der Suche nach speziellen Lösungen verliert.\n",
    "\n",
    "`hvplot` erzeugt `holoviews`-Objekte, die sich nach einem [ausgeklügelten Schema](https://holoviews.org/user_guide/Building_Composite_Objects.html) zusammensetzen lassen. Wir kratzen hier auch nur an der Oberfläche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60582dc-0078-475e-a9c6-f6fa3229e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(width=1200, grid=True, xformatter=datefmt)\n",
    "plot1 = df.T01.hvplot(**opts) \n",
    "plot2 = df.T03.hvplot(**opts)\n",
    "(plot1 + plot2).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 * plot2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a427c-4104-41c2-9394-2e78e3e89b66",
   "metadata": {},
   "source": [
    "Je größer die Daten sind, desto nützlicher wird die Visualisierung in zwei Dimensionen, z.B. als Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2f390-fff8-4a9e-afca-5569bc07aec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(df.index.hour).mean().hvplot.heatmap(ylabel=\"hour of day\", cmap=\"RdYlBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb96a64-1184-4f35-8014-41da365eddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.T01 - df.T06).rename(\"diff\").hvplot.heatmap(\n",
    "    x=\"time.day_of_year\",\n",
    "    y=\"time.hour\",\n",
    "    C=\"diff\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    symmetric=True,\n",
    "    clabel=\"T01 – T06 [°C]\",\n",
    ").aggregate(function=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b330e-6291-4056-a672-7dc122bf89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.T01 - df.T06).rename(\"diff\").hvplot.heatmap(\n",
    "    x=\"time.day_of_year\",\n",
    "    y=\"time.hour\",\n",
    "    C=\"diff\",\n",
    "    cmap=\"reds\",\n",
    "    clabel=\"RMSD (T01 – T06) [°C]\",\n",
    ").aggregate(function=lambda x: np.sqrt(np.mean(np.square(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec34a16-2adc-4572-b835-285ef084c3cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Die Daten der beiden DataFrames sollen nun zusammengefügt werden. Die Zeitstempel scheinen in etwa im 3min-Raster aufgenommen worden zu sein. Wir überprüfen das wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5303e-47db-404c-a11f-5ee0b69b4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots = [\n",
    "    pd.DataFrame(np.diff(mydf.index.values.astype(np.int64)) / 1e9)\n",
    "    .clip(150, 200)\n",
    "    .hvplot.hist(bins=100, xlabel=\"Messintervall [s]\")\n",
    "    for mydf in [df, sot]\n",
    "]\n",
    "hv.Layout(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a35063-3444-4ab6-960b-96850e2ea7b6",
   "metadata": {},
   "source": [
    "Hierbei wurde ausgenutzt, dass die Zeitstempel (im Kern `numpy.datetime64`) intern als Nanosekunden ab \"Epochenbeginn\" vorliegen. Sie lassen sich also leicht in Integerwerte umwandeln, mit denen dann gerechnet werden kann. **Bei größeren Datumsmanipulation in Pandas kann das deutlich schneller sein als mit `pd.Timestamp`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc47c0c-043d-450a-9564-91c9c5d2586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sot.index[0], type(sot.index[0]))\n",
    "print(sot.index.values[0], type(sot.index.values[0]))\n",
    "sec = sot.index.values[0].astype(\"i8\") / 1e9\n",
    "print(sec, \"Sekunden\")\n",
    "print(pd.Timestamp(sec * 1e9))\n",
    "print(sec.astype(\"M8[s]\"))  # mit [s] als sekundengenau interpretiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc87874-ef12-448d-b639-6df15cba2db3",
   "metadata": {},
   "source": [
    "Wir möchten nun alle Daten auf einen gemeinsamen Zeitindex mit 3min-Intervall interpolieren. Hier stößt Pandas aufgrund seiner Herkunft (aus den Wirtschaftswissenschaften!) an die Grenzen: `.interpolate` füllt nur bestehende NaNs auf, `.resample(\"3min\").mean().interpolate()` ist nicht korrekt, weil dann in manchen Intervallen zwei Werte gemittelt werden, in manchen gar keine. Das kann man überprüfen mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3553285-17c8-487b-bff1-89b2abec351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample(\"3min\").mean().isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375c119-a2f8-4d88-8bd6-fa7768909257",
   "metadata": {
    "tags": []
   },
   "source": [
    "Erzeugen wir zunächst einen neuen, gemeinsamen Zeitindex mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2649cb-4d84-407f-83c0-39d9fbf17286",
   "metadata": {},
   "outputs": [],
   "source": [
    "newidx = pd.date_range(\n",
    "    max(df.index[0], sot.index[0]).floor(\"h\"),\n",
    "    min(df.index[-1], sot.index[-1]),\n",
    "    freq=\"3min\",\n",
    ")\n",
    "newidx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47331b-1a7a-4b64-8546-e61bd366ba4b",
   "metadata": {},
   "source": [
    "Dann können wir den ursprünglichen DataFrame mit den neuen Indices ergänzen, die enstehenden NaNs interpolieren und anschließend alles wieder auf den neuen Index einschränken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1b5b3-f155-43bb-85aa-f164ce546c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [df.drop(columns=\"name\"), pd.DataFrame(0, index=newidx, columns=[\"_dummy\"])], axis=1\n",
    ").interpolate().drop(columns=\"_dummy\").reindex(newidx).dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a0a3d-0c3b-4075-ad4b-3682f10d8e4a",
   "metadata": {},
   "source": [
    "O weh! Das muss doch irgendwie einfacher gehen??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13078de-89fb-40ae-8d76-129cbd9f0858",
   "metadata": {},
   "source": [
    "## Mehrdimensionale Daten - `xarray`\n",
    "\n",
    "`xarray` setzt auf Pandas auf und ermöglicht eine \"naturwissenschaftlichere\" Betrachtungsweise der Daten als ein- oder mehrdimensionale `DataArray`s, deren Dimensionen mit **Koordinatenachsen** versehen sind. Ein Pandas `DataFrame` kann auf natürliche Weise in eine Kollektion von `DataArray`s überführt werden. Das enstehende `Dataset` verhält sich bezüglich der Variablen wieder in etwa wie ein `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4495c75-7b59-4737-aacc-a7117358f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = (\n",
    "    sot.drop(columns=[\"time\", \"date\", \"name\", \"anlage\"])\n",
    "    .to_xarray()\n",
    "    .rename(index=\"time\")\n",
    ")\n",
    "xa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cfb4a-3933-446d-825a-c06236f5c512",
   "metadata": {},
   "source": [
    "Als Metadaten hat man hier:\n",
    "\n",
    "+ `dims`: Dimensionsnamen für jede Koordinatenachse. \n",
    "+ `coords`: Ein `dict`-ähnlicher Container von (typischerweise) 1-dimensionalen Koordinatenvektoren für die einzelnen Dimensionen.\n",
    "+ `attrs`: Ein `OrderedDict` mit beliebigen Metadaten. Bestimmte standardisierte Keys (z.B. `\"units\"`) werden direkt beim Plotten verwendet.\n",
    "\n",
    "Einer der Vorteile von `xarray` ist dadurch, dass der Zwang aufgehoben wird, alle Metainformationen über eine Variable in den Spaltennamen zu pressen – der möglichst auch noch Python-konform sein soll, damit man ihn über ein Attribut referenzieren kann. Ein Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081059f-a13f-417d-bd91-a916fb04c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa.time.attrs[\"long_name\"] = \"Messzeitpunkt\"\n",
    "xa.TemperaturVL.attrs[\"long_name\"] = \"Vorlauftemperatur\"\n",
    "xa.TemperaturVL.attrs[\"units\"] = \"°C\"\n",
    "t = xa.TemperaturVL\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464f250-4155-44b0-b505-71b3fad6c263",
   "metadata": {},
   "source": [
    "Bei Bedarf können natürlich noch andere Informationen wie Messpunkt-ID, Genauigkeit usw. als Attribute gesetzt werden. Damit ist diese Messgröße vollständig beschrieben und bedarf keiner externen Dokumentation mehr!\n",
    "\n",
    "Das Plotten erfolgt ganz analog zu Pandas. Man beachte die automatische Verwendung der Metadaten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d841cdc-2f44-4484-8e55-0446d9bfe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:100].hvplot(label=\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff969e-65a4-4fe4-9729-bf41561de9b2",
   "metadata": {},
   "source": [
    "Lineare Interpolation von Daten ist nun mit einem einzelnen Methodenaufruf möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f81aa6-85aa-4b69-ab15-755f6de13b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.hvplot(label=\"original\") * t.interp(time=newidx).hvplot(label=\"interpoliert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31a898-b3fa-40c3-850c-7a97233301d9",
   "metadata": {},
   "source": [
    "Mit der Methode `xr.combine_by_coordinates` lassen sich verschiedene `Datasets` automatische zusammenfügen, auch in mehreren Dimensionen gleichzeitig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf913333-f702-416e-a337-5cde822a8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.combine_by_coords(\n",
    "    [xa.interp(time=newidx), df.drop(columns=\"name\").to_xarray()], data_vars=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2157b-28da-4a09-8451-bce4a99dfdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = xr.load_dataset(\"wind_fc.nc\")\n",
    "wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af98d9-2545-4123-93f2-0ec5a4c5bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp1 = wp.isel(time=slice(None, 100), fc=slice(100, None))\n",
    "wp2 = wp.isel(time=slice(150, 200), fc=slice(0, 50))\n",
    "xr.combine_by_coords([wp2, wp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190a58b-f4b4-48bc-88c8-178e4320176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.targ.hvplot.image() + xr.combine_by_coords([wp2, wp1]).targ.hvplot.image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d6ec4-feaf-4652-afc2-1a4075cef41f",
   "metadata": {},
   "source": [
    "Aggregation erfolgt ebenfalls analog zu Pandas. Es können auch mehrere Dimensionen angegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f2f71-794f-45d3-875e-7eb1919dd2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.mean(dim=\"time\").hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd125ec9-8db7-4313-b654-6dd2775be4a7",
   "metadata": {},
   "source": [
    "Werden für einen Plot nicht alle Dimensionen verwendet, erzeugt `hvplot` (normalerweise) ein Widget, mit dem die fehlenden Dimensionen interaktiv eingestellt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b4b28-6d0e-47a0-8d62-cdad2044a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.hvplot(x=\"fc\", ylabel=f\"wind speed [m/s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d243679-e413-476a-95fb-bba9ec82be8e",
   "metadata": {},
   "source": [
    "Eine praktische Option für divergierende Colormaps ist `symmetric`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088dcc4-27f2-4462-964f-f4b7838047fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = (\n",
    "    (wp.pred - wp.targ)\n",
    "    .rename(\"wind speed bias\")\n",
    "    .hvplot.heatmap(\n",
    "        x=\"time.day\",\n",
    "        y=\"time.hour\",\n",
    "        C=\"wind speed bias\",\n",
    "        cmap=\"RdBu_r\",\n",
    "        symmetric=True,\n",
    "        clabel=\"m/s\",\n",
    "    )\n",
    ")\n",
    "myplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a9e38-8d8a-493f-97e0-a3a74de039f5",
   "metadata": {},
   "source": [
    "### Hilfskoordinaten\n",
    "\n",
    "Nehmen wir an, wir möchten den Fehler der Vorhersage zu jeder Tageszeit bestimmen. Da `time` nur eindimensional ist, können wir sie nicht verwenden, um jeder Stunde einen Fehler zuzuweisen. Wir bauen daher eine zweidimensionale Hilfskoordinate.\n",
    "\n",
    "Da wir für jeden Vorhersagewert eine `hr_of_day` benötigen, zählen wir erst die Vorhersagestunde hinzu und nehmen das ganze dann Modulo 24. Das funktioniert, wenn der erste Zeitstempel um 00:00 ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b73663-414d-4edc-8c3d-f40a39c8cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_since_begin = (wp.time - wp.time[0]).astype(\n",
    "    int\n",
    ") / 3.6e12  # ns -> h\n",
    "dtime = sum(np.meshgrid(wp.fc, hr_since_begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d2b48-b8f8-4bb6-a3c1-c25480058ef1",
   "metadata": {},
   "source": [
    "Das Ergebnis benutzen wir als 2D-Hilfskoordinate und erstellen auch noch eine Tageskoordinate, \n",
    "damit wir ganz einfach Statistik betreiben können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbca06d-0aa3-4fcd-a6c4-2745d35b2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = wp.assign_coords(\n",
    "    hr_of_day=((\"time\", \"fc\"), dtime.astype(int) % 24),\n",
    "    day=((\"time\",), wp.time.dt.floor(\"D\").data),\n",
    "    hr_since_begin=((\"time\",), hr_since_begin.data),\n",
    ")\n",
    "wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3712f-9d74-4956-933a-e5914894af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wp.pred - wp.targ).groupby(\"hr_of_day\").apply(\n",
    "    lambda x: np.sqrt(np.mean(np.square(x)))\n",
    ").rename(\"RMSE [m/s]\").hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59987e8-9c49-4732-b8bf-b9d10b5f2c77",
   "metadata": {},
   "source": [
    "## Abspeichern hochaufgelöster Plots\n",
    "\n",
    "Typischerweise spielt `hvplot` seine Stärken mehr in einer interaktiven Umgebung aus. Anbei der Vollständigkeit halber noch ein Trick, um `holoviews`-Plots (keine Layouts!) als SVG und PNG in hoher Auflösung zu exportieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32223d-a1cb-4b8a-9d77-dd85acda59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hack to export SVG\n",
    "from subprocess import run\n",
    "\n",
    "from bokeh.io import export_svg as bokeh_export\n",
    "\n",
    "\n",
    "def savepic(obj, filename, dpi=200):\n",
    "    plot_state = hv.renderer(\"bokeh\").get_plot(obj).state\n",
    "    plot_state.output_backend = \"svg\"\n",
    "    filepath = f\"{filename}.svg\"\n",
    "    bokeh_export(plot_state, filename=filepath)\n",
    "    run(\n",
    "        f\"convert -density {dpi} {filepath} {filepath.replace('.svg', '.png')}\",\n",
    "        shell=True,\n",
    "    )\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89651823-ff18-4b8d-8a18-ebef6eedf925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#savepic(myplot, \"testplot\")  # funktioniert hier nicht, wegen fehlender Pakete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e160f-acb3-4c1d-b195-4b5e36c31053",
   "metadata": {},
   "source": [
    "## Effiziente Datenformate\n",
    "\n",
    "Für größere, numerische Datenmengen empfielt es sich nicht, alles in CSV-Dateien abzuspeichern, da derartige ASCII-Formate sehr ineffizient sind. Moderne binäre Datenformate sind durch **chunking**, **lazy loading** und **distributed access** häufig um ein Vielfaches schneller.\n",
    "\n",
    "1. **chunking** bedeutet, dass die Daten nicht als einzelne Datei auf der Festplatte liegen, sondern als intelligent aufgeteilte Blöcke. Die Laderoutinen erlauben dann selektives Laden eines Teils der Daten, bei dem nur die benötigten Blöcke tatsächlich gelesen werden.\n",
    "2. Beim **lazy loading** werden beim Aufrufen der Laderoutine nur die Metadaten eingelesen. Der Rest folgt erst, wenn tatsächlich auf die Daten zugegriffen wird.\n",
    "3. **distributed access** meint das gleichzeitige Laden mehrerer Blöcke in verschiedenen Prozessen oder Threads, mit für den User transparenter Zusammenführung im Speicher.\n",
    "\n",
    "### Pandas\n",
    "\n",
    "**`.to_hdf`** erlaubt das Speichern von `DataFrames` im HDF5-Format mittels des `pytables`-Pakets. Dazu sollte die Option `pd.set_option('io.hdf.default_format','table')` gesetzt werden; das Defaultformat `fixed` ist eher veraltet. HDF5 ist auch nicht wirklich \"modern\" im Sinne der oben beschriebenen Kriterien, aber es unterstützt Kompression, mehrere `DataFrames` in einer Datei sowie das Anhängen von Daten an bestehende Files.\n",
    "\n",
    "**`.to_parquet`** ist eine modernere und effizientere Alternative, die zumindest die Kriterien 1 und 2 erfüllt. Parquet nutzt spaltenweise, schnelle Komprimierungsalgorithmen. Das Anfügen an bestehende Daten wird nicht nativ von `.to_parquet` unterstützt, ist aber mit einem kleinen Hack möglich.\n",
    "\n",
    "### xarray\n",
    "\n",
    "**`.to_netcdf`** ist der Standard für `xarray`. Es wird ein netCDF4-File (setzt auf HDF5 auf) geschrieben, das bei der Wahl entprechender Attribute dem [CF-Conventions-Standard](https://cfconventions.org/) entspricht. NetCDF4 selber erfüllt die obigen Kriterien nicht, aber `xarray` erlaubt sozusagen \"manuelles\" chunking, indem die Routine `.open_mfdataset` eine beliebige Liste von Files gleichzeitig öffnen und anhand der enthaltenen Dimensionen in ein `Dataset` kombinieren kann. In Kombination mit dem `dask`-Paket lassen sich so Pipelines bauen, bei denen nie die ganzen Daten gleichzeitig im Speicher sind. \n",
    "\n",
    "**`.to_zarr`** ist ein neueres Feature und bietet automatisches chunking sowie die Möglichkeit, die Daten direkt in diversen Cloud-Speichern abzulagen. Auch ansonsten ist das Format Parquet sehr ähnlich, erlaubt aber auch das Anhängen von Daten entlang einer Dimension. Ein Beispiel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964fa83-2aec-4b34-9926-d9632cde2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"temp{np.random.randint(999999):0d}\"\n",
    "ds0 = xr.Dataset(\n",
    "    {\"temperature\": ([\"time\"], [50, 51, 52])},\n",
    "    coords={\"time\": pd.date_range(\"2000-01-01\", periods=3)},\n",
    ")\n",
    "ds1 = xr.Dataset(\n",
    "    {\"temperature\": ([\"time\"], [53, 54, 55])},\n",
    "    coords={\"time\": pd.date_range(\"2000-01-04\", periods=3)},\n",
    ")\n",
    "\n",
    "ds0.to_zarr(filename)\n",
    "ds1.to_zarr(filename, mode=\"a\", append_dim=\"time\")\n",
    "\n",
    "ds2 = xr.open_zarr(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8c1d4-0f98-4454-ae13-66a68b65c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49142220-163d-4b4f-befb-d4a5ba2c862b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
