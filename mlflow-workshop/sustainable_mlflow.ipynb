{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9f81ac",
   "metadata": {},
   "source": [
    "# SustAInable MLflow Workshop\n",
    "\n",
    "Willkommen! In diesem Notebook bauen wir einen kompakten **Train–Test–Deploy**-Workflow mit **MLflow** – inkl. **CO₂-Tracking** mit CodeCarbon und inline-UI.\n",
    "\n",
    "**Ablauf:**\n",
    "1. **Setup & UI** – Lokales Tracking (SQLite), MLflow-UI im Notebook  \n",
    "2. **Daten** – Gesamtdatensatz & Train/Val/Test-Splits loggen  \n",
    "3. **Training** – RandomForest + Metriken, Artefakte, Modell  \n",
    "4. **CO₂-Tracking** – Emissionen pro Run messen  \n",
    "5. **HPO** – Parent-Run + Child-Runs vergleichen  \n",
    "6. **Anreicherung** – Feature Importances nachträglich loggen  \n",
    "7. **Registry** – Bestes Modell registrieren & auf *Staging* promoten  \n",
    "8. **Serving** – REST-API starten & Predictions testen  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b87de3",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "**Nötige Environment aufsetzen**\n",
    "# Optional – ausführen, falls noch nicht getan\n",
    "\n",
    "Per one-liner:\n",
    "```bash\n",
    "conda create -n sustainable-mlflow python=3.10 \\\n",
    "  numpy pandas matplotlib scikit-learn mlflow \\\n",
    "  pytorch torchvision torchaudio cpuonly \\\n",
    "  plotly requests ipywidgets notebook codecarbon \\\n",
    "  -c pytorch -c conda-forge\n",
    "```\n",
    "oder per env.yml:\n",
    "```bash\n",
    "conda env create -f env.yml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0978111",
   "metadata": {},
   "source": [
    "**Die Biliotheken Importieren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b96525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/codecarbon/input.py:9: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/codecarbon/core/gpu.py:22: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml\n"
     ]
    }
   ],
   "source": [
    "# Die nötigen Bibliotheken importieren\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests, json\n",
    "import subprocess, time\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07408550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7dc350",
   "metadata": {},
   "source": [
    "\n",
    "## 1) MLflow konfigurieren (lokal, SQLite mit Model Registry)\n",
    "Wir nutzen eine **SQLite‑Datenbank** als Tracking‑Backend – Das ist aber nur eine von vielen Möglichkeiten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74918e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lokale SQLite-DB (Datei wird im aktuellen Arbeitsverzeichnis angelegt)\n",
    "MLFLOW_DB = \"sqlite:///mlflow_workshop.db\"\n",
    "mlflow.set_tracking_uri(MLFLOW_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8dd13a",
   "metadata": {},
   "source": [
    "## 1.1) MFLow GUI öffnen\n",
    "\n",
    "In der GUI wir sehen was passiert ist, nicht zwingend notwendig. Aber sehr hilfreich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/29 21:29:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:29:46 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "INFO  [alembic.runtime.migration] Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:29:47 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:29:47 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "[2025-09-29 21:29:47 +0200] [1084730] [INFO] Starting gunicorn 23.0.0\n",
      "[2025-09-29 21:29:47 +0200] [1084730] [INFO] Listening at: http://127.0.0.1:5003 (1084730)\n",
      "[2025-09-29 21:29:47 +0200] [1084730] [INFO] Using worker: sync\n",
      "[2025-09-29 21:29:47 +0200] [1084731] [INFO] Booting worker with pid: 1084731\n",
      "[2025-09-29 21:29:47 +0200] [1084732] [INFO] Booting worker with pid: 1084732\n",
      "[2025-09-29 21:29:47 +0200] [1084733] [INFO] Booting worker with pid: 1084733\n",
      "[2025-09-29 21:29:47 +0200] [1084734] [INFO] Booting worker with pid: 1084734\n",
      "2025/09/29 21:30:03 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:30:03 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:30:13 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:30:13 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "# disable new registry UI\n",
    "os.environ[\"MLFLOW_ENABLE_NEW_REGISTRY_UI\"] = \"false\"\n",
    "\n",
    "# Startet die MLflow UI im Hintergrund auf Port 5003 (standard ist 5000)\n",
    "# mit der SQLite DB als Backend-Store, falls man seine Daten wo anders speichert muss der Pfad angepasst werden\n",
    "ui_proc = subprocess.Popen(\n",
    "    [\"mlflow\", \"ui\", \"--backend-store-uri\", \"sqlite:///mlflow_workshop.db\", \"-p\", \"5003\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027509df",
   "metadata": {},
   "source": [
    "Server kann jetzt über den Port 5003 (http://127.0.0.1:5003) im Browser gesehen werden.\n",
    "Falls gewünscht zeigt es die nächste Zeile direkt im Notebook an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676edcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://127.0.0.1:5003\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd986a358d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/29 21:30:33 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:30:33 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:30:35 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:30:35 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:30:52 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:30:52 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "# Warten bis Server bereit ist\n",
    "time.sleep(3)\n",
    "\n",
    "# Anzeige als iFrame\n",
    "display(IFrame(\"http://127.0.0.1:5003\", width=\"100%\", height=1000))\n",
    "\n",
    "# Hinweis: Am Ende des Workshops nicht vergessen zu stoppen mit:\n",
    "# ui_proc.terminate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785a4ed",
   "metadata": {},
   "source": [
    "## 2) Experiment anlegen\n",
    "\n",
    "MlFlow bei Tracking immer alles in Runs.\n",
    "Ein Experiment ist eine Sammlung von Runs.\n",
    "Ein Experiment hat eine eindeutige ID selbst wenn andere mit dem gleichen Namen erzeugt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6014a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/29 21:31:51 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:31:51 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:31:51 INFO mlflow.tracking.fluent: Experiment with name 'sustAInable-mlflow-workshop' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: 1\n",
      "Tracking URI: sqlite:///mlflow_workshop.db\n",
      "Experiment: sustAInable-mlflow-workshop\n"
     ]
    }
   ],
   "source": [
    "# Einheitlicher Experiment-Name für alle Runs\n",
    "EXPERIMENT_NAME = \"sustAInable-mlflow-workshop\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "expereriment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "\n",
    "print(\"Experiment ID:\", expereriment_id)\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219622b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Datensatz: Simpler PV-Datensatz\n",
    "Daten sind über auch über die URL verfügbar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67171c",
   "metadata": {},
   "source": [
    "#### 3.1) Datensatz als ganzes Eintrage\n",
    "\n",
    " Vorteil: Erzeugter Datenhash ist direkt mit dem der Quelle vergleichbar ohne erneutes Speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071b59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz laden\n",
    "df = pd.read_csv(\"pv_data.csv\")\n",
    "\n",
    "# Daten zu run hinzufügen\n",
    "TARGET = \"Erzeugung[Wh] t+24\"\n",
    "SOURCE = \"./pv_data.csv\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"PV Vorhersage - Daten\") as run:\n",
    "    ds_train = mlflow.data.from_pandas(\n",
    "        df, source=f\"{SOURCE}\", name=\"pv-data\", targets=TARGET\n",
    "    )\n",
    "    mlflow.log_input(ds_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a3b59",
   "metadata": {},
   "source": [
    "#### 3.2) Datensatz Trainings, Validations und Testsplits eintragen\n",
    "\n",
    "Vorteil: Änderungen in den Split sind schneller offensichtlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac33be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_splits(run_name, nested=False):\n",
    "    with mlflow.start_run(run_name=run_name, nested=nested) as run:\n",
    "        for split in [\"training\", \"validation\", \"test\"]:\n",
    "            df = pd.read_csv(f\"pv_data-{split}.csv\")\n",
    "            df = mlflow.data.from_pandas(\n",
    "                df, source=f\"pv_data-{split}.csv\", name=f\"pv-data-{split}\", targets=TARGET\n",
    "            )\n",
    "            mlflow.log_input(df, context=split)\n",
    "        run_id = run.info.run_id\n",
    "        return run_id\n",
    "\n",
    "run_name = \"PV Vorhersage - Daten Splits\"\n",
    "run_id = log_splits(run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7170c1",
   "metadata": {},
   "source": [
    "## 4) **Training**\n",
    "\n",
    "Wir werden über alle Experimente immer mit der gleichen Funktion das Model trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de215586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition Trainingsfunktion\n",
    "def train_model(run_name, params):\n",
    "        \n",
    "        ds_train = pd.read_csv(\"pv_data-training.csv\")\n",
    "        # build features/target for training\n",
    "        X_train = ds_train.drop(columns=TARGET)\n",
    "        y_train = ds_train[TARGET]\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        # Modell trainieren\n",
    "        model = RandomForestRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        # Modell loggen\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            name=\"random-forest-model\",\n",
    "            #registered_model_name=\"RandomForestPVModel\",\n",
    "            input_example=X_train.iloc[:5],\n",
    "            signature=mlflow.models.infer_signature(X_train, y_train_pred),\n",
    "        )\n",
    "\n",
    "\n",
    "        for split in [\"training\", \"validation\", \"test\"]:\n",
    "            # Trainingsdaten laden\n",
    "            ds = pd.read_csv(f\"pv_data-{split}.csv\")\n",
    "            X = ds.drop(columns=TARGET)\n",
    "            y = ds[TARGET]\n",
    "            # Vorhersagen machen\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            # Metriken berechnen\n",
    "            rmse = root_mean_squared_error(y, y_pred)\n",
    "\n",
    "            mlflow.log_metric(f\"rmse_{split}\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918f0d0",
   "metadata": {},
   "source": [
    "#### 4.1) Einzelnes Modell trainieren\n",
    "Wir trainieren ein einzelnes Random Forest Modell. Wir loggen:\n",
    "- **Parameter** (Anzahle der Bäume, Tiefe der Bäume, festen Zufallswert)\n",
    "- **Metriken** (RMSE auf Train/Val/Test)\n",
    "- **Modell** inkl. Signatur (für spätere Nutzung/Serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261f6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/29 21:35:51 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:35:51 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:35:51 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:35:51 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 21:36:06 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 21:36:06 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "run_name = \"PV Vorhersage - Einzel Modell\"\n",
    "# Defintion der Paramter für den Run\n",
    "params = {\"n_estimators\": 10, \"max_depth\": 3, \"random_state\": 7}\n",
    "\n",
    "run_id = log_splits(run_name)\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    train_model(run_name, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55fc02",
   "metadata": {},
   "source": [
    "#### 4.2) C02 Footprint\n",
    "Zusätzlich loggen wir den C02 Footprint des Trainings. Hierfür nutzen wir die CodeCarbon-Bibliothek.\n",
    "Es gibt dazu viele Alternativen, z.B:\n",
    " - CarbonTracker\n",
    " - Experiment-Impact-Tracker (EIT)\n",
    " - Green-Algorithms\n",
    " - Eco2AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc217c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:38:29] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:38:29] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:38:29] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:38:29] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:38:29] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:38:29] CPU Model on constant consumption mode: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:38:29] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:38:29]   Platform system: Linux-6.13.4-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "[codecarbon INFO @ 21:38:29]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 21:38:29]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 21:38:29]   Available RAM : 31.246 GB\n",
      "[codecarbon INFO @ 21:38:29]   CPU count: 16\n",
      "[codecarbon INFO @ 21:38:29]   CPU model: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:38:29]   GPU count: 1\n",
      "[codecarbon INFO @ 21:38:29]   GPU model: 1 x NVIDIA GeForce RTX 4080 SUPER\n",
      "[codecarbon INFO @ 21:38:35] Energy consumed for RAM : 0.000010 kWh. RAM Power : 11.717405319213867 W\n",
      "[codecarbon INFO @ 21:38:35] Energy consumed for all GPUs : 0.000019 kWh. Total GPU Power : 23.101 W\n",
      "[codecarbon INFO @ 21:38:35] Energy consumed for all CPUs : 0.000027 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 21:38:35] 0.000056 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "run_name = \"PV Vorhersage - CO2 Footprint\"\n",
    "# Defintion der Paramter für den Run\n",
    "params = {\"n_estimators\": 10, \"max_depth\": 3, \"random_state\": 7}\n",
    "\n",
    "#######################\n",
    "# CO2 Tracking\n",
    "tacker = EmissionsTracker()\n",
    "tacker.start()\n",
    "\n",
    "\n",
    "run_id = log_splits(run_name)\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    train_model(run_name, params)\n",
    "\n",
    "    ########################\n",
    "    # CO2 Tracking\n",
    "    tacker.stop()\n",
    "    mlflow.log_metric(\"co2_footprint_kg\", tacker.final_emissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a62d2b",
   "metadata": {},
   "source": [
    "#### 4.3) Hyperparamteroptimierung und Unterruns\n",
    "\n",
    "Wir wollten eine ganze Versuchsreihe an Experimenten trainieren und diese strukturiert speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88284d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:40:28] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:40:28] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:40:28] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:40:28] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:40:28] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:40:28] CPU Model on constant consumption mode: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:28] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:40:28]   Platform system: Linux-6.13.4-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "[codecarbon INFO @ 21:40:28]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 21:40:28]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 21:40:28]   Available RAM : 31.246 GB\n",
      "[codecarbon INFO @ 21:40:28]   CPU count: 16\n",
      "[codecarbon INFO @ 21:40:28]   CPU model: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:28]   GPU count: 1\n",
      "[codecarbon INFO @ 21:40:28]   GPU model: 1 x NVIDIA GeForce RTX 4080 SUPER\n",
      "[codecarbon INFO @ 21:40:31] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:40:31] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:40:31] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:40:31] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:40:31] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:40:31] CPU Model on constant consumption mode: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:31] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:40:31]   Platform system: Linux-6.13.4-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "[codecarbon INFO @ 21:40:31]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 21:40:31]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 21:40:31]   Available RAM : 31.246 GB\n",
      "[codecarbon INFO @ 21:40:31]   CPU count: 16\n",
      "[codecarbon INFO @ 21:40:31]   CPU model: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:31]   GPU count: 1\n",
      "[codecarbon INFO @ 21:40:31]   GPU model: 1 x NVIDIA GeForce RTX 4080 SUPER\n",
      "[codecarbon INFO @ 21:40:36] Energy consumed for RAM : 0.000006 kWh. RAM Power : 11.717405319213867 W\n",
      "[codecarbon INFO @ 21:40:36] Energy consumed for all GPUs : 0.000011 kWh. Total GPU Power : 22.663 W\n",
      "[codecarbon INFO @ 21:40:36] Energy consumed for all CPUs : 0.000016 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 21:40:36] 0.000033 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:36] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:40:36] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:40:36] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:40:36] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:40:36] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:40:36] CPU Model on constant consumption mode: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:37] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:40:37]   Platform system: Linux-6.13.4-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "[codecarbon INFO @ 21:40:37]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 21:40:37]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 21:40:37]   Available RAM : 31.246 GB\n",
      "[codecarbon INFO @ 21:40:37]   CPU count: 16\n",
      "[codecarbon INFO @ 21:40:37]   CPU model: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:37]   GPU count: 1\n",
      "[codecarbon INFO @ 21:40:37]   GPU model: 1 x NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterrun 1 mit 20 Bäumen, max_depth 3 abgeschlossen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:40:42] Energy consumed for RAM : 0.000007 kWh. RAM Power : 11.717405319213867 W\n",
      "[codecarbon INFO @ 21:40:42] Energy consumed for all GPUs : 0.000014 kWh. Total GPU Power : 23.101 W\n",
      "[codecarbon INFO @ 21:40:42] Energy consumed for all CPUs : 0.000019 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 21:40:42] 0.000040 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:42] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:40:42] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:40:42] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:40:42] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:40:42] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:40:42] CPU Model on constant consumption mode: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:42] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:40:42]   Platform system: Linux-6.13.4-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "[codecarbon INFO @ 21:40:42]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 21:40:42]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 21:40:42]   Available RAM : 31.246 GB\n",
      "[codecarbon INFO @ 21:40:42]   CPU count: 16\n",
      "[codecarbon INFO @ 21:40:42]   CPU model: AMD Ryzen 7 3700X 8-Core Processor\n",
      "[codecarbon INFO @ 21:40:42]   GPU count: 1\n",
      "[codecarbon INFO @ 21:40:42]   GPU model: 1 x NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterrun 2 mit 50 Bäumen, max_depth 5 abgeschlossen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:40:46] Energy consumed for RAM : 0.000049 kWh. RAM Power : 11.717405319213867 W\n",
      "[codecarbon INFO @ 21:40:46] Energy consumed for all GPUs : 0.000098 kWh. Total GPU Power : 23.469 W\n",
      "[codecarbon INFO @ 21:40:46] Energy consumed for all CPUs : 0.000135 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 21:40:46] 0.000282 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:50] Energy consumed for RAM : 0.000016 kWh. RAM Power : 11.717405319213867 W\n",
      "[codecarbon INFO @ 21:40:50] Energy consumed for all GPUs : 0.000031 kWh. Total GPU Power : 22.360000000000003 W\n",
      "[codecarbon INFO @ 21:40:50] Energy consumed for all CPUs : 0.000045 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 21:40:50] 0.000093 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:50] Energy consumed for RAM : 0.000062 kWh. RAM Power : 11.717405319213867 W\n",
      "[codecarbon INFO @ 21:40:50] Energy consumed for all GPUs : 0.000122 kWh. Total GPU Power : 22.360000000000003 W\n",
      "[codecarbon INFO @ 21:40:50] Energy consumed for all CPUs : 0.000171 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 21:40:50] 0.000355 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterrun 3 mit 200 Bäumen, max_depth 15 abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "run_name = \"PV Vorhersage - CO2 Footprint\"\n",
    "# Defintion der Paramter für den Run\n",
    "params = {\"n_estimators\": 100, \"max_depth\": 10, \"random_state\": 7}\n",
    "\n",
    "run_id = log_splits(run_name)\n",
    "tacker = EmissionsTracker()\n",
    "tacker.start()\n",
    "# Eltern run starten\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    # 3 Unterruns mit verschiedenen Parametern\n",
    "    for run_num, n_estimators, max_depth, random_state in [(1, 20, 3, 7), (2, 50, 5, 7), (3, 200, 15, 7)]:\n",
    "        params = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "        # Unterrun starten\n",
    "        local_tacker = EmissionsTracker()\n",
    "        local_tacker.start()\n",
    "        run_name=f\"Unterrun_{run_num}\"\n",
    "        child_run_id = log_splits(run_name,nested=True)\n",
    "        with mlflow.start_run(run_id=child_run_id, nested=True) as child_run:\n",
    "            train_model(run_name, params)\n",
    "            local_tacker.stop()\n",
    "            mlflow.log_metric(\"co2_footprint_kg\", local_tacker.final_emissions)\n",
    "\n",
    "        print(f\"Unterrun {run_num} mit {n_estimators} Bäumen, max_depth {max_depth} abgeschlossen.\")\n",
    "    tacker.stop()\n",
    "    mlflow.log_metric(\"co2_footprint_kg\", tacker.final_emissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24604d8",
   "metadata": {},
   "source": [
    "#### 4.4) Zusätzliche Evaluationsmöglichkeiten hinzufügen\n",
    "\n",
    "Wir wollten eine ganze Versuchsreihe an Experimenten nachträglich noch mit Feature-Importance Plots anreichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63a9e037",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m X_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[TARGET])    \u001b[38;5;66;03m# features only\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 3) prepare artifact dir & write CSV\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m art_dir \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(mlflow\u001b[38;5;241m.\u001b[39mget_artifact_uri()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     24\u001b[0m art_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    child_runs = mlflow.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.parentRunId = '{run_id}'\",\n",
    "        order_by=[\"metrics.rmse_validation ASC\"],\n",
    "    )\n",
    "\n",
    "    for child_run_id in child_runs.run_id:\n",
    "        # attach to the existing child run so artifacts land there\n",
    "        with mlflow.start_run(run_id=child_run_id, nested=True):\n",
    "            # 1) load the model from this child run\n",
    "            model_uri = f\"runs:/{child_run_id}/random-forest-model\"\n",
    "            model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "            # 2) get feature names from training split\n",
    "            df_train = pd.read_csv(\"pv_data-training.csv\")\n",
    "            X_train = df_train.drop(columns=[TARGET])    # features only\n",
    "\n",
    "            # 3) prepare artifact dir & write CSV\n",
    "            art_dir = Path(mlflow.get_artifact_uri().replace(\"file://\", \"\"))\n",
    "            art_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            importances = getattr(model, \"feature_importances_\", None)\n",
    "            if importances is None:\n",
    "                raise AttributeError(\"Model has no feature_importances_. Did you log a tree model?\")\n",
    "\n",
    "            names = list(getattr(X_train, \"columns\", [f\"x{j}\" for j in range(len(importances))]))\n",
    "            out_path = art_dir / \"feature_importances.csv\"\n",
    "\n",
    "            pd.DataFrame({\"feature\": names, \"importance\": importances}) \\\n",
    "              .sort_values(\"importance\", ascending=False) \\\n",
    "              .to_csv(out_path, index=False)\n",
    "\n",
    "            mlflow.log_artifact(str(out_path))\n",
    "\n",
    "        # ---- Plot\n",
    "        sorted_idx = importances.argsort()[::-1]  # descending\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(importances)), importances[sorted_idx], align=\"center\")\n",
    "        plt.xticks(range(len(importances)), [names[i] for i in sorted_idx], rotation=90)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_png = art_dir / \"feature_importances.png\"\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(str(out_png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86319cc6",
   "metadata": {},
   "source": [
    "\n",
    "## 5) **Deploy‑Vorbereitung** – Bestes Modell in die **Model Registry**\n",
    "Wir registrieren das beste Modell als `RegisteredModel` und promoten es in **Staging**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a44c46",
   "metadata": {},
   "source": [
    "#### 5.1) Runs vergleichen und bestes Modell finden\n",
    "In der MLflow‑UI könnt ihr visuell vergleichen. Hier zeigen wir zusätzlich, wie man **programmatisch** den besten Run (min `rmse_val`) auswählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "941ff71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run: f27269bd4e3b49c28eb531a26212c888 RMSE: 470.9532302172802\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "child_runs = mlflow.search_runs(\n",
    "    experiment_ids=[exp.experiment_id],\n",
    "    filter_string=f\"tags.mlflow.parentRunId = '{run_id}'\",\n",
    "    order_by=[\"metrics.rmse_validation ASC\"],\n",
    ")\n",
    "\n",
    "best_row = child_runs.iloc[0]\n",
    "best_run_id = best_row[\"run_id\"]\n",
    "\n",
    "# metric column names in search_runs DF are prefixed with \"metrics.\"\n",
    "metric_col = (\n",
    "    \"metrics.rmse_validation\"\n",
    "    if \"metrics.rmse_validation\" in child_runs.columns\n",
    "    else \"metrics.rmse_val\"\n",
    ")\n",
    "\n",
    "best_rmse = best_row[metric_col]\n",
    "print(\"Best run:\", best_run_id, \"RMSE:\", best_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a5427",
   "metadata": {},
   "source": [
    "#### 5.2) Das beste Model registrieren\n",
    "Wir nutzen den Staging Tag das Model noch über die REST API getestet werden muss bevor wir es auf Produktive setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "881c925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'sustainable-random-forest' already exists. Creating a new version of this model...\n",
      "2025/09/29 02:13:20 WARNING mlflow.tracking._model_registry.fluent: Run with id f27269bd4e3b49c28eb531a26212c888 has no artifacts at artifact path 'random-forest-model', registering model based on models:/m-e7ab70b948b14e35a38693f4d189622c instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered: sustainable-random-forest version: 1\n",
      "Promoted to: Staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'sustainable-random-forest'.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "REGISTERED_NAME = \"sustainable-random-forest\"\n",
    "\n",
    "# Modellartefakt-Pfad des besten Runs\n",
    "model_uri = f\"runs:/{best_run_id}/random-forest-model\"\n",
    "\n",
    "# Registrierung (legt ggf. automatisch die erste Version an)\n",
    "mv = mlflow.register_model(model_uri=model_uri, name=REGISTERED_NAME)\n",
    "print(\"Registered:\", mv.name, \"version:\", mv.version)\n",
    "\n",
    "# In STAGING promoten\n",
    "client.transition_model_version_stage(\n",
    "    name=REGISTERED_NAME,\n",
    "    version=mv.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False,\n",
    ")\n",
    "print(\"Promoted to:\", \"Staging\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19091987",
   "metadata": {},
   "source": [
    "\n",
    "## 6) **Serve** – REST‑API starten & Prediction testen\n",
    "\n",
    "Wir testen erstmal mit unserem Notebook ob das Model über den REST-Server funktioniert, falls ja, setzen wir es auf Produktion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea2052",
   "metadata": {},
   "source": [
    "##### 6.1.2 Server starten (Python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2dccfc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/29 02:13:28 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 02:13:28 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 02:13:28 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 02:13:28 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/29 02:13:28 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2025/09/29 02:13:28 INFO mlflow.pyfunc.backend: === Running command 'exec uvicorn --host 127.0.0.1 --port 5001 --workers 1 mlflow.pyfunc.scoring_server.app:app'\n",
      "2025/09/29 02:13:30 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/29 02:13:30 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO:     Started server process [1035515]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 98] error while attempting to bind on address ('127.0.0.1', 5001): address already in use\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/bin/mlflow\", line 11, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/click/core.py\", line 1462, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/click/core.py\", line 1383, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/click/core.py\", line 1850, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/click/core.py\", line 1850, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/click/core.py\", line 1246, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/click/core.py\", line 814, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/mlflow/models/cli.py\", line 102, in serve\n",
      "    return get_flavor_backend(\n",
      "  File \"/home/kai/.conda/envs/sustainable-mlflow/lib/python3.10/site-packages/mlflow/pyfunc/backend.py\", line 322, in serve\n",
      "    raise Exception(\n",
      "Exception: Command '['bash', '-c', 'exec uvicorn --host 127.0.0.1 --port 5001 --workers 1 mlflow.pyfunc.scoring_server.app:app']' returned non zero return code. Return code = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model server is running at http://127.0.0.1:5001\n",
      "Send POST requests to /invocations\n"
     ]
    }
   ],
   "source": [
    "import subprocess, time\n",
    "\n",
    "# Name of the registered model\n",
    "REGISTERED_NAME = \"sustainable-random-forest\"\n",
    "\n",
    "# Start serving the \"Staging\" version in background\n",
    "serve_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"mlflow\", \"models\", \"serve\",\n",
    "        \"-m\", f\"models:/{REGISTERED_NAME}/Staging\",\n",
    "        \"-p\", \"5001\", \"--no-conda\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# give server some time to start\n",
    "time.sleep(5)\n",
    "print(\"Model server is running at http://127.0.0.1:5001\")\n",
    "print(\"Send POST requests to /invocations\")\n",
    "\n",
    "# When you're done:\n",
    "# serve_proc.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "824022cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request payload: {\"dataframe_split\": {\"columns\": [\"Stunde des Tages sin\", \"Stunde des Tages cos\", \"Tag des Jahres sin\", \"Tag des Jahres cos\", \"Erzeugung[Wh] t-1\", \"Erzeugung[Wh] t-3\", \"Erzeugung[Wh] t-6\"], \"data\": [[-0.887885218402376, 0.460065037731152, 0.952117665910714, -0.305731827359753, 491.0, 2166.0, 3509.0], [0.979084087682323, 0.203456013052634, 0.88545602565321, 0.464723172043769, 0.0, 0.0, 0.0], [0.631087944326053, -0.77571129070442, -0.79247684195109, -0.609902004400073, 550.0, 165.0, 0.0]]}}\n",
      "Status: 200\n",
      "Response: {\"predictions\": [37.26823660237449, 1.1928851058436225, 1256.064442565918]}\n",
      "Original y_test: [  21.    0. 1345.]\n",
      "Terminal command to test with curl:\n",
      "curl -X POST http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d ' {\"dataframe_split\": {\"columns\": [\"Stunde des Tages sin\", \"Stunde des Tages cos\", \"Tag des Jahres sin\", \"Tag des Jahres cos\", \"Erzeugung[Wh] t-1\", \"Erzeugung[Wh] t-3\", \"Erzeugung[Wh] t-6\"], \"data\": [[-0.887885218402376, 0.460065037731152, 0.952117665910714, -0.305731827359753, 491.0, 2166.0, 3509.0], [0.979084087682323, 0.203456013052634, 0.88545602565321, 0.464723172043769, 0.0, 0.0, 0.0], [0.631087944326053, -0.77571129070442, -0.79247684195109, -0.609902004400073, 550.0, 165.0, 0.0]]}} '\n"
     ]
    }
   ],
   "source": [
    "# Test the REST API with a few samples from the test set\n",
    "MODEL_URI = \"models:/sustainable-random-forest/Staging\"  # adjust to your model\n",
    "model = mlflow.pyfunc.load_model(MODEL_URI)\n",
    "\n",
    "input_schema = model.metadata.get_input_schema()\n",
    "# Try to pull expected DataFrame column names (if any)\n",
    "df_cols = [c.name for c in getattr(input_schema, \"inputs\", []) if hasattr(c, \"name\") and c.name]\n",
    "\n",
    "# Load test data (same as used during training)\n",
    "ds_test =  pd.read_csv(\"pv_data-test.csv\")\n",
    "X_test = ds_test.drop(columns=[TARGET])\n",
    "y_test = ds_test[TARGET]\n",
    "\n",
    "row = X_test[27:30]  # shape (1, d)\n",
    "# Ensure dtype float (some schemas enforce double/float)\n",
    "row = np.asarray(row, dtype=float)\n",
    "\n",
    "if df_cols:  # Model expects a DataFrame with named columns\n",
    "    # If your X_test has different column order/count -> align or stop with a clear error\n",
    "    if row.shape[1] != len(df_cols):\n",
    "        raise ValueError(f\"Feature count mismatch: payload has {row.shape[1]} cols, \"\n",
    "                         f\"model expects {len(df_cols)}: {df_cols[:5]}...\")\n",
    "    payload = {\"dataframe_split\": {\"columns\": df_cols, \"data\": row.tolist()}}\n",
    "else:        # Model expects a tensor (list-of-lists), no column names\n",
    "    # Infer expected feature count from tensor spec (last dim)\n",
    "    tspec = input_schema.inputs[0]  # TensorSpec\n",
    "    expected = tspec.shape[-1] if tspec.shape and tspec.shape[-1] is not None else row.shape[1]\n",
    "    if row.shape[1] != expected:\n",
    "        raise ValueError(f\"Feature count mismatch: payload has {row.shape[1]}, model expects {expected}.\")\n",
    "    payload = {\"inputs\": row.tolist()}\n",
    "\n",
    "res = requests.post(\n",
    "    \"http://127.0.0.1:5001/invocations\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=json.dumps(payload),\n",
    ")\n",
    "print(\"Request payload:\", json.dumps(payload)[:500])\n",
    "print(\"Status:\", res.status_code)\n",
    "print(\"Response:\", res.text[:500])\n",
    "\n",
    "print(\"Original y_test:\", y_test[27:30].values)\n",
    "\n",
    "print(\"Terminal command to test with curl:\")\n",
    "print(\"curl -X POST http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '\", json.dumps(payload), \"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f549803",
   "metadata": {},
   "source": [
    "#### 6.2 Terminalversion\n",
    "Die Modelle können jetzt auf einem Inference Server über das Terminal zur Verfügung gestellt werden\n",
    "> **Hinweis:** Serving bitte **im Terminal** starten. Öffnet ein zweites Terminal im selben Ordner.\n",
    "\n",
    "Dieses mal mit dem Production Tag. Dafür in der UI das Model auf Production setzten\n",
    "(Dafür die neue UI Ansicht ausschalten)\n",
    "\n",
    "##### 6.2.1 Server starten (Terminal)\n",
    "\n",
    "```bash\n",
    "conda activate sustainable-mlflow\n",
    "export MLFLOW_TRACKING_URI=sqlite:///mlflow_workshop.db\n",
    "mlflow models serve -m \"models:/{REGISTERED_NAME}/Production\" -p 5002 --no-conda\n",
    "```\n",
    "\n",
    "Also in unserem Fall ()\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m \"models:/sustainable-random-forest/Staging\" -p 5002 --no-conda --host 0.0.0.0\n",
    "```\n",
    "\n",
    "Das \"--host 0.0.0.0\" erlaubt uns von extern darauf zuzugreigen\n",
    "\n",
    "\n",
    "*Alternativ* könnt ihr auch eine konkrete Version serven:\n",
    "```bash\n",
    "mlflow models serve -m \"models:/{REGISTERED_NAME}/1\" -p 5002 --no-conda\n",
    "```\n",
    "\n",
    "##### 6.2.2 Prediction aufrufen (Terminal)\n",
    "```bash\n",
    "conda activate sustainable-mlflow\n",
    "curl -X POST http://127.0.0.1:5002/invocations \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"dataframe_split\":{\"columns\":[%COLS%],\"data\":[%ROW%]}}'\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda activate sustainable-mlflow\n",
    "curl -X POST http://127.0.0.1:5002/invocations -H 'Content-Type: application/json' -d ' {\"dataframe_split\": {\"columns\": [\"Stunde des Tages sin\", \"Stunde des Tages cos\", \"Tag des Jahres sin\", \"Tag des Jahres cos\", \"Erzeugung[Wh] t-1\", \"Erzeugung[Wh] t-3\", \"Erzeugung[Wh] t-6\"], \"data\": [[-0.887885218402376, 0.460065037731152, 0.952117665910714, -0.305731827359753, 491.0, 2166.0, 3509.0], [0.979084087682323, 0.203456013052634, 0.88545602565321, 0.464723172043769, 0.0, 0.0, 0.0], [0.631087944326053, -0.77571129070442, -0.79247684195109, -0.609902004400073, 550.0, 165.0, 0.0]]}} '\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sustainable-mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
